{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6c2122-0a30-4d08-8133-f672499a445f",
   "metadata": {},
   "source": [
    "# $T_{bg}$ estimate\n",
    "\n",
    "In this notebook we compare the variability metrics computed on the references'answers with the ones computed on the models under test to estimate their $T_{bg}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f552e6-11a0-4f2b-a3f0-651493d25188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from scipy.stats import ks_2samp\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import comb\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8dbc0-7bff-4f20-aebf-d4f37b424550",
   "metadata": {},
   "source": [
    "### Variability Metrics and K-S\n",
    "In the following cells we define the variability metrics considered and we implement the Kolmogorov-Smirnov distance used to measure the distance between distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5b928-7b97-467e-a011-e2ec08943835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum exact match fraction\n",
    "def exact_match_fraction(answers_model):\n",
    "    freq_max = []\n",
    "    for question, answers in answers_model.items():\n",
    "        n_answers = len(answers)\n",
    "        answers_as_tuples = [tuple(ans) for ans in answers]\n",
    "        \n",
    "        counter = Counter(answers_as_tuples)\n",
    "        \n",
    "        most_common_answer, freq = counter.most_common(1)[0]\n",
    "        \n",
    "        #freq_max[question] = freq / n_answers\n",
    "        freq_max.append(freq / n_answers)\n",
    "    return freq_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2b64c-8ce1-4530-822b-ab4d7a6287a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein distance\n",
    "def Levenshtein(answers_model, max_tokens):\n",
    "    lev = []\n",
    "    for question, answers in answers_model.items():\n",
    "        n = len(answers)\n",
    "        matrix_lev = np.zeros((n, n), dtype=float)\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                matrix_lev[i, j] = editdistance.eval(tuple(answers[i]), tuple(answers[j]))/max_tokens\n",
    "        triu_vals = matrix_lev[np.triu_indices(n, k=1)]\n",
    "        #norma = np.linalg.norm(matrix_lev)/(n*n)\n",
    "        lev.append(np.mean(triu_vals))\n",
    "    return lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c30bb-3b4a-4c53-8aa4-7b310a9e32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_lcs(a, b):\n",
    "    # Computes the LCS length using a patience-sortingâ€“style algorithm for sequences with potentially repeated elements.\n",
    "    pos = defaultdict(list)\n",
    "    for i, x in enumerate(b):\n",
    "        pos[x].append(i)\n",
    "\n",
    "    seq = []\n",
    "    for x in a:\n",
    "        for j in reversed(pos.get(x, [])):\n",
    "            idx = bisect.bisect_left(seq, j)\n",
    "            if idx == len(seq):\n",
    "                seq.append(j)\n",
    "            else:\n",
    "                seq[idx] = j\n",
    "    return len(seq)\n",
    "\n",
    "\n",
    "def average_lcs_distance(answers_model, use_fast_lcs=True):\n",
    "    \"\"\"\n",
    "    Computes the average pairwise LCS-based distance between answers\n",
    "    for each question, including identical-answer pairs.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "\n",
    "    for question, answers in answers_model.items():\n",
    "        n = len(answers)\n",
    "        if n <= 1:\n",
    "            distances.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Deduplicate answers to reduce computations while preserving correct weighting of all original answer pairs.\n",
    "        answers_as_tuples = [tuple(a) for a in answers]\n",
    "        counter = Counter(answers_as_tuples)\n",
    "        unique_answers = list(counter.keys())\n",
    "        freqs = list(counter.values())\n",
    "        K = len(unique_answers)\n",
    "\n",
    "        # Compute the total weighted distance over all unique answer pairs.\n",
    "        total_weighted_distance = 0.0\n",
    "\n",
    "        for i in range(K):\n",
    "            a = unique_answers[i]\n",
    "            fa = freqs[i]\n",
    "            for j in range(i, K):\n",
    "                b = unique_answers[j]\n",
    "                fb = freqs[j]\n",
    "\n",
    "                # Identical answers have zero distance.\n",
    "                if i == j:\n",
    "                    d = 0.0\n",
    "                else:\n",
    "                    # Select LCS implementation (fast or standard DP).\n",
    "                    if use_fast_lcs:\n",
    "                        L = fast_lcs(a, b)\n",
    "                    else:\n",
    "                        m, n_ = len(a), len(b)\n",
    "                        dp = [[0]*(n_+1) for _ in range(m+1)]\n",
    "                        for ii in range(m):\n",
    "                            for jj in range(n_):\n",
    "                                if a[ii] == b[jj]:\n",
    "                                    dp[ii+1][jj+1] = dp[ii][jj] + 1\n",
    "                                else:\n",
    "                                    dp[ii+1][jj+1] = max(dp[ii+1][jj], dp[ii][jj+1])\n",
    "                        L = dp[m][n_]\n",
    "                    d = 1 - L / min(len(a), len(b))\n",
    "\n",
    "                # Number of original answer pairs represented by this pair.\n",
    "                if i == j:\n",
    "                    weight = fa * (fa - 1) / 2 \n",
    "                else:\n",
    "                    weight = fa * fb\n",
    "\n",
    "                total_weighted_distance += d * weight\n",
    "\n",
    "        # Normalize by the total number of answer pairs.\n",
    "        total_pairs = comb(sum(freqs), 2)\n",
    "        distances.append(total_weighted_distance / total_pairs)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df984cf-fb42-4666-8a3c-7ec95b79737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorv-Smirnov distance\n",
    "def ks_distance(list1, list2):\n",
    "    ks_stat, _ = ks_2samp(list1, list2)\n",
    "    return ks_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f72d1-8d4f-4712-b76b-ddaa62c4a383",
   "metadata": {},
   "source": [
    "### Loading responses from reference models and target models for $T_{bg}$  estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6df37-bdfa-4652-b1f3-2d7b94d5c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"references.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    references = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905973df-4415-435d-bc29-d08c2aa79994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert in the following dictionary the names of the file with the asnwers given by the models whose T_bg is being estimated\n",
    "tested_dict = {\"gpt-4.1-nano_Azure\":\"answers_gpt.json\",\n",
    "               \"claude-sonnet-4_AWS\":\"answers_claudesonnet4.json\",\n",
    "               \"grok-3-mini_AWS\": \"answers_grok.json\"}\n",
    "\n",
    "tested = {}\n",
    "for key in tested_dict:\n",
    "    with open(tested_dict[key], \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    tested[key] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091bfb0-abb5-400b-ac69-c809b44be9a8",
   "metadata": {},
   "source": [
    "### Compute variability distributions\n",
    "Below we compute the variability distributions both for the reference models and for the models whose $T_{bg}$ is being estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92411f57-f063-4e1f-8926-290d15749132",
   "metadata": {},
   "outputs": [],
   "source": [
    "variabilities = [\"max_exact_match\", \"Levenshtein_lists\", \"average_lcs_distance\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450fcb7-c60e-4a40-9286-29bd83fc4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_variabilities = {}\n",
    "for key in references:\n",
    "    print(key)\n",
    "    ref_variabilities[key] = {}\n",
    "    for var in variabilities:\n",
    "        print(var)\n",
    "        ref_variabilities[key][var] = {}\n",
    "        for temp in tqdm(references[key]):\n",
    "            temp_r = round(float(temp), 2)\n",
    "            if var == \"max_exact_match\":\n",
    "                ref_variabilities[key][var][temp_r] = exact_match_fraction(references[key][temp])\n",
    "            elif var == \"Levenshtein_lists\":\n",
    "                ref_variabilities[key][var][temp_r] = Levenshtein(references[key][temp], 32)\n",
    "            elif var == \"average_lcs_distance\":\n",
    "                ref_variabilities[key][var][temp_r] = average_lcs_distance(references[key][temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cc50d-d34a-4eeb-a4c9-c5a09dcfca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variabilities = {}\n",
    "for key in tested.keys():\n",
    "    test_variabilities[key] = {}\n",
    "    for var in variabilities:\n",
    "        if var == \"max_exact_match\":\n",
    "            test_variabilities[key][var] = exact_match_fraction(tested[key])\n",
    "        elif var == \"Levenshtein_lists\":\n",
    "            test_variabilities[key][var] = Levenshtein(tested[key], 32)           \n",
    "        elif var == \"average_lcs_distance\":\n",
    "            test_variabilities[key][var] = average_lcs_distance(tested[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b7612-49db-4056-8ec7-d214c874dbdd",
   "metadata": {},
   "source": [
    "### Estimate of $T_{bg}$\n",
    "Below, we compare the variability distributions computed for the models under study with the reference ones, thereby estimating $T_{bg}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60490143-4908-4c0a-9d1e-68c637e3f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only common questions - in the case in which the datasets of the references and of the models under test are different\n",
    "index_common = {}\n",
    "for ref in ref_variabilities.keys():\n",
    "    index_common[ref] = {}\n",
    "    ref_questions = list(references[ref][list(references[ref].keys())[0]].keys())\n",
    "    for mod in test_variabilities.keys():\n",
    "        test_questions = list(tested[mod].keys())\n",
    "        index_common[ref][mod] = [i for i, x in enumerate(ref_questions) if x in test_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8193bbb-19d4-44d7-98ef-6e9989b23209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the K-S distance between the variability distributions computed on the answers given by the models under test \n",
    "#and the variability distributions of the answers given by the reference models at the sampled temperatures.\n",
    "distances = {}\n",
    "for mod in test_variabilities.keys():\n",
    "    distances[mod] = {}\n",
    "    for ref in ref_variabilities.keys():\n",
    "        distances[mod][ref] = {}\n",
    "        distances[mod][ref]['num_prompt'] = len(index_common[ref][mod])\n",
    "        for var in variabilities:\n",
    "            dists = {}\n",
    "            for temp in ref_variabilities[ref][var].keys():\n",
    "                dists[temp] = ks_distance(test_variabilities[mod][var], [ref_variabilities[ref][var][temp][i] for i in index_common[ref][mod]])\n",
    "                distances[mod][ref][var] = dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee28f0b-33f9-47d3-93ec-dda3859451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the temperatures that minimize the K-S distances computed previously for each reference model and model under test\n",
    "results = {}\n",
    "for test_mod, subdict in distances.items():\n",
    "    results[test_mod] = {}\n",
    "    for ref_mod, metrics in subdict.items():\n",
    "        results[test_mod][ref_mod] = {}\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if isinstance(metric_values, dict):  # solo metriche con coppie (temp, valore)\n",
    "                min_val = min(metric_values.values())\n",
    "                min_temps = [t for t, v in metric_values.items() if v == min_val]\n",
    "                results[test_mod][ref_mod][metric_name] = {\n",
    "                    \"min_value\": min_val,\n",
    "                    \"temperatures\": min_temps,\n",
    "                    \"num_prompts\": distances[test_mod][ref_mod]['num_prompt']\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f59fcf-5b84-4b8f-9eb0-9c97b62ab7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_rows = []\n",
    "\n",
    "for model1, subdict in results.items():\n",
    "    row = {\"Tested\": model1}\n",
    "    num_prompts = None  \n",
    "\n",
    "    for model2, metrics in subdict.items():\n",
    "        for metric_name, info in metrics.items():\n",
    "            name_ref = model2.split(\"-\")[0]\n",
    "            col_prefix = f\"{name_ref}_{metric_name}\"\n",
    "            temps = info[\"temperatures\"]\n",
    "            avg_temp = sum(temps) / len(temps) if temps else None\n",
    "\n",
    "            if num_prompts is None and \"num_prompts\" in info:\n",
    "                num_prompts = info[\"num_prompts\"]\n",
    "\n",
    "            # Save the mean temperatures for each couple variability metric / reference model\n",
    "            row[f\"{col_prefix}_Tn\"] = avg_temp\n",
    "\n",
    "    row[\"Num_prompts\"] = num_prompts\n",
    "    flat_rows.append(row)\n",
    "\n",
    "df_flat = pd.DataFrame(flat_rows)\n",
    "\n",
    "# Compure the for each variability metric\n",
    "metric_cols = [c for c in df_flat.columns if \"_Tn\" in c]\n",
    "\n",
    "for metric_name in variabilities:\n",
    "    cols = [c for c in metric_cols if metric_name in c]\n",
    "    df_flat[f\"Mean_{metric_name}\"] = df_flat[cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# Overall mean of the estimates obtained for each variability metric\n",
    "df_flat[\"Tbg estimate\"] = df_flat[metric_cols].mean(axis=1, skipna=True)\n",
    "\n",
    "# Reordering columns\n",
    "first_cols = [\"Tested\", \"Tbg estimate\", \"Mean_max_exact_match\", \"Mean_Levenshtein_lists\",'Mean_average_lcs_distance', \"Num_prompts\"]\n",
    "other_cols = [c for c in df_flat.columns if c not in first_cols]\n",
    "df_flat = df_flat[first_cols + sorted(other_cols)]\n",
    "\n",
    "# Saving in a csv all the datas\n",
    "df_flat.to_csv(\"results_Tbg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bb1be-7094-4596-ae52-232ee5c48c5d",
   "metadata": {},
   "source": [
    "The table below (with the data saved in the csv above) reports, for each model, the final Tbg estimate, the values of the individual variability metrics, and the number of prompts used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008be63-0a76-4176-a926-d177863248b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_main = [\n",
    "    \"Tested\",\n",
    "    \"Tbg estimate\",\n",
    "    \"Mean_max_exact_match\",\n",
    "    \"Mean_Levenshtein_lists\",\n",
    "    \"Mean_average_lcs_distance\",\n",
    "    \"Num_prompts\",\n",
    "]\n",
    "\n",
    "df_flat[cols_main].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49209242-3def-47e8-8756-2408df263a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
