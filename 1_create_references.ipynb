{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e249ae-611a-49d2-8c6c-9ca8ac3ef984",
   "metadata": {},
   "source": [
    "# Generation of Reference Distributions ($f_{T}^{l}$)\n",
    "\n",
    "This notebook implements the protocol described in Section 5 to generate the reference answers used to estimate the background temperature. We collect outputs from a set of idealized reference models $\\mathcal{L}$ (SmolLM3-3B, Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3) over a grid of temperatures. These outputs form the empirical distributions $f_{T}^{l}, l \\in \\mathcal{L}$, which serve as the baselines to measure the background temperature of the system under test.\n",
    "\n",
    "Note that the models and dataset used here serve as an example, other models (in a stable inference environment) and dataset could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de020e6-a4c6-420a-890c-c8ccf44d8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca51c6-7e16-4e47-afa0-d2c9caee1dae",
   "metadata": {},
   "source": [
    "### Reference Models Setup\n",
    "As defined in the experimental settings, we use locally executed models to ensure a stable inference environment. \n",
    "- Models: Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, SmolLM3-3B.\n",
    "- Stability: These models are run on local GPUs to minimize system-level non-determinism, serving as the \"quasi-ideal\" baseline described in Section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c030381-2863-4321-99ea-bbc3d741c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_models = [\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "                    \"HuggingFaceTB/SmolLM3-3B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686aa80-de4f-4d2c-b93b-441dc52ecc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_prompt(model_name, prompt):\n",
    "    if model_name == \"HuggingFaceTB/SmolLM3-3B\":\n",
    "        messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "    elif model_name == \"/opt/data/IIPV/experiments/NLP/LanguageModels/Llama-3.2-3B-Instruct\":  #\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "        messages = [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "    elif model_name == \"/opt/data/IIPV/experiments/NLP/LanguageModels/Mistral-7B-Instruct-v0.3\": #\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        messages = [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "    else:\n",
    "        messages = [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6cb03-4685-491b-894b-6d40059875f6",
   "metadata": {},
   "source": [
    "### Setup dell'Esperimento e Generazione delle Risposte\n",
    "\n",
    "The following cells define the dataset ($\\Pi^{30}$) and sampling temperatures ($\\Theta$). Then we launch and save the actual generation of answers by the reference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f4966-f9c6-4c26-abac-bd7475cd0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = list(np.arange(0.0, 0.2, 0.01)) + list(np.arange(0.2, 0.5 + 0.05, 0.05)) + [0.6,0.7, 0.8,0.9, 1] \n",
    "temperatures = [round(t,2) for t in temperatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994c30f-6da8-4dae-9774-83576d2ddcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_dataset(\"truthfulqa/truthful_qa\", 'generation')['validation']['question'][:30]\n",
    "N = 32\n",
    "tok_limit = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1ea49-3300-4ede-9b16-0e7ab3403f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "references = {}    #dictionary that will contain all the answers for each model, temperature and prompt.\n",
    "\n",
    "for model_name in reference_models:\n",
    "    print(model_name)\n",
    "    references[model_name] = {}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    for temperature in tqdm(temperatures):\n",
    "        references[model_name][temperature] = {}\n",
    "        for question in prompts:\n",
    "            answers = []\n",
    "            messages = messages_prompt(model_name, question)            \n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            L = len(model_inputs.input_ids[0])\n",
    "            for i in range(N):\n",
    "                try:\n",
    "                    #some model does not give the possibility of set the temperature to 0, but set top_k=1 is equivalent.\n",
    "                    if temperature == 0:\n",
    "                        generated_ids = model.generate(**model_inputs, max_new_tokens=tok_limit, top_k = 1)\n",
    "                    else:\n",
    "                        generated_ids = model.generate(**model_inputs, max_new_tokens=tok_limit, temperature = temperature)\n",
    "                    answers.append(generated_ids[0][L:].tolist())\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            references[model_name][temperature][question] = answers\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872f005-ded8-480c-92ae-312b22dd4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"references.json\", \"w\") as f:\n",
    "    json.dump(references, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a8e30-ddbc-4d33-ba03-497123a2162e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
